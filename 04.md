お気づきの方がいるかはわかりませんが、ブログタイトルを変えました。  
同研究室([小町研究室][])の[@Ace12358][]君に感化されました。([自然言語処理のウェザーリポート][])[@zawa9510][]君もブログを始めたようです。([自然言語処理の三ツ星カルテット][])

今回は[Capter02][](pdf)、n-gram言語モデルです。この辺はさくさくっと復習してHMMやパーセプトロンあたりをじっくりやるつもりだったのですが、意外と四苦八苦しています。  

前回は1-gramを学びましたが、他にも2-gram, 3-gram, ..., n-gram, ...とあります。((ちなみに、1-gram, 2-gram, 3-gramはそれぞれ 「ユニグラム」, 「バイグラム」, 「トライグラム」と読みます。表記も「unigram」, 「bigram」, 「trigram」とすることがあります。  
それ以上は「よんグラム」とか「フォーグラム」とか適当です。表記も大抵「4-gram」, 「5-gram」です。))(実際使われるのは5-gramくらいまで？)これらをまとめた考えがn-gramです。単に「n-gramで」といった場合はnがいくつなのか考えたほうが良いでしょう。


今回もスライドに沿って進めていきますが、「前回の復習」は前回の記事を読めばいいので基本的に飛ばしていきます。

### 1-gram言語モデルの特徴
前回勉強した1-gram言語モデルですが、1-gram言語モデルにはその他のn-gram(2-gram, 3-gram, ...)に比べていくつかの特徴があります。それは、

1. 語順を考慮しない
2. 単語の関係性を考慮しない

という点です。
「語順を考慮しない」について見てみると、

> P<sub>uni</sub>(w = <font color="red">speech</font> <font color="blue">recognition</font> <font color="green">system</font>) =  
　 P(w=<font color="red">speech</font>) \* P(w=<font color="blue">recognition</font>) \* P(w=<font color="green">system</font>) \* P(w=&lt;/s>)  
 P<sub>uni</sub>(w = <font color="green">system</font> <font color="blue">recognition</font> <font color="red">speech</font>) =  
　 P(w=<font color="red">speech</font>) \* P(w=<font color="blue">recognition</font>) \* P(w=<font color="green">system</font>) \* P(w=&lt;/s>)  

のように、上の2つの単語列は同じ確率になります。考えてみれば当たり前ですが、それぞれの単語の確率を掛けあわせているだけなので順番は関係ありません。  
「単語の関係性を考慮しない」については、

> P<sub>uni</sub>(w = <font color="blue">i am</font>) =
P(w=i) \* P(w=am) \* P(w=&lt;/s>)  
P<sub>uni</sub>(w = <font color="blue">we are</font>) =
P(w=we) \* P(w=are) \* P(w=&lt;/s>)  
<br>
> P<sub>uni</sub>(w = <font color="red">we am</font>) =
P(w=we) \* P(w=am) \* P(w=&lt;/s>)  
P<sub>uni</sub>(w = <font color="red">i are</font>) =
P(w=i) \* P(w=are) \* P(w=&lt;/s>)  

のように、文法的でない文(名詞と活用が矛盾)に対しても文法的な文(名詞と活用が一致)に対してもほぼ同等の確率を与えています。((P(we)とP(i), P(are)とP(am)がそれぞれほぼ同等の確率になる(ことを仮定している)ため))  
このように、1-gramでは語順や語の関係性を考慮しないという特徴があります。

### n-gram言語モデル
上に示したように1-gram言語モデルでは、語順や語の関係性を考慮しませんでした。一方、2-gram以上のn-gram言語モデルを用いると以下に示すように文脈を考慮するので、語順や語の関係性も考慮することができます。

> ・1-gram言語モデルは文脈を考慮しない(単語の確率のみ)  
　　P(w<sub>i</sub> | w<sub>0</sub> ... w<sub>i-1</sub>) ≈ P(w<sub>i</sub>)  

> ・2-gram言語モデルは1単語の文脈を考慮(2単語の並びの確率)  
　　P(w<sub>i</sub> | w<sub>0</sub> ... w<sub>i-1</sub>) ≈ P(w<sub>i</sub> | w<sub>i-1</sub>)  

> ・3-gram言語モデルは2単語の文脈を考慮(3単語の並びの確率)  
　　P(w<sub>i</sub> | w<sub>0</sub> ... w<sub>i-1</sub>) ≈ P(w<sub>i</sub> | w<sub>i-2</sub>w<sub>i-1</sub>)  
・4-gram, 5-gram,...

#### n-gram確率
1-gram確率では最尤推定における履歴を考えませんでした。n-gram確率では全く考えないわけではなく、そのn-gramが考慮する文脈(n-1単語)までの履歴を考えます。2-gramであれば1単語の文脈を考慮するので、1単語分の履歴まで考えます。  

つまり、n-gram確率はn単語とn-1単語からなる文字列の頻度をそれぞれカウントすればよいのです。((n-1単語)の頻度 / (n単語)の頻度)
><div style="text-align: center;">[tex:{ \displaystyle
P(w\_i|w\_{i-n+1}\ldots w\_{i-1}) = \frac{c(w\_{i-n+1}...w\_i)}{c(w\_{i-n+1}...w\_{i-1})}
}]</div>

例えばn=2, すなわち2-gramの場合は次のように計算します。
> i live in osaka .  
i am a graduate student .  
my school is in nara .  
<br>
> P(osaka | in) = c(in osaka)/c(in) = 1/2 = 0.5  
P(nara | in) = c(in nara)/c(in) = 1/2 = 0.5

### 低頻度n-gramの問題
上の例で使ったコーパスにおいて`in school`の単語列を考えた時に、1-gram言語モデルでは確率を算出することができます。しかし、2-gramではゼロになってしまいます。`P(school | in) = c(in school)/c(in) = 0/2 = 0`
1-gramでもあったようにn-gramにも低頻度の問題があるのです。もっと言えば、n-gramの場合はnが大きいほど低頻度の問題が発生しやすいです。長い履歴を考慮しようとすればするほど、コーパスに存在しない単語列が出てくるからです。  

この問題の解決にも1-gramのときに利用したInterpolation(線形補間)を利用します。  
1-gramの補間には一定の割合で未知語の確率を足していました。2-gramでは一定の確率で1-gramの確率を足します。さらに3-gramでは2-gramの確率を...というように、Interpolationでは低次のn-gramの確率を足し合わせて補間することで低頻度の問題を解決します。  

> 3-gram : P(w<sub>i</sub> | w<sub>i-2</sub>w<sub>i-1</sub>) = λ<sub>3</sub>P<sub>ML</sub>(w<sub>i</sub> | w<sub>i-2</sub>w<sub>i-1</sub>) + (1-λ<sub>3</sub>)P(w<sub>i</sub> | w<sub>i-1</sub>)  
2-gram : P(w<sub>i</sub> | w<sub>i-1</sub>) = λ<sub>2</sub>P<sub>ML</sub>(w<sub>i</sub> | w<sub>i-1</sub>) + (1-λ<sub>2</sub>)P(w<sub>i</sub>)  
1-gram : P(w<sub>i</sub>) = λ<sub>1</sub>P<sub>ML</sub>(w<sub>i</sub>) + (1-λ<sub>1</sub>) * 1/N

1-gramの補間には補間係数λ<sub>1</sub>を利用しました。同様に2-gramの補間にはλ<sub>2</sub>を、3-gramの補間にはλ<sub>3</sub>を...それぞれ補間係数として利用します。  
また、前回も言及したようにInterpolation以外にも様々なsmoothingの方法があります。実装の都合上、今回はBack-off smoothingについても触れます。

###補間係数の選択
前回の1-gram言語モデルでは補間係数λ<sub>1</sub>の値は適当に`0.95`と決めていました。実際には、このようなパラメータはチューニングを行って決定します。チューニングにはいくつかの方法があるのですが、ここでは「グリッド探索」と「Witten-Bell Smoosing」を勉強します。

#####グリッド探索
グリッド探索は様々な係数の組み合わせを試し、尤度が最も高くなるように選択します。  
2-gramの場合、補間係数は2つあるので次のような組み合わせをすべて試してみます。
> <div style="text-align: center;">
λ<sub>2</sub> = 0.95,　λ<sub>1</sub> = 0.95  
λ<sub>2</sub> = 0.95,　λ<sub>1</sub> = 0.90  
λ<sub>2</sub> = 0.95,　λ<sub>1</sub> = 0.85  
・・・  
λ<sub>2</sub> = 0.95,　λ<sub>1</sub> = 0.05  
λ<sub>2</sub> = 0.90,　λ<sub>1</sub> = 0.95  
λ<sub>2</sub> = 0.90,　λ<sub>1</sub> = 0.90  
・・・  
λ<sub>2</sub> = 0.05,　λ<sub>1</sub> = 0.10  
λ<sub>2</sub> = 0.05,　λ<sub>1</sub> = 0.05  
</div>

すべて試した結果、一番良い評価が得られた組み合わせを実際に採用します(尤度が高いということ)。しかし、この場合選択肢が多すぎて係数の選択に時間がかかります。更に、(nの数が同一な)すべてのn-gramに対して同じ係数を考えていますが、これで尤度が最適になるとは限りません。（同じ2-gramだからといって`P(school|in)`と`P(bus|school)P`の補間係数を同じにするのがベストとは限らない）  

#####Witten-Bell Smoothing

そこで、文脈を考慮した補間係数の選択を考えます。例えば、頻度の高い単語`Tokyo`と頻度の低い単語`Tottori`では、

<table align="center">
	<tr>
		<td>

c(Tokyo city) = 40<br>
c(Tokyo is) = 35<br>
c(Tokyo was) = 24<br>
c(Tokyo tower) = 15<br>
c(Tokyo port) = 10<br>
<div style="text-align: center;">
・<br>
・
</div>
		</td>
		<td>

c(Tottori is) = 2<br>
c(Tottori city) = 1<br>
<font color=#888888>c(Tottori was) = 0</font>  
		</td> 
	</tr>
</table>

ほとんどの2-gramが既観測である`Tokyo`には大きな補間係数を、未観測の2-gramが多い`Tottori`には小さな補間係数を選択するのが最適だと思われます。以下の式に表されるように、補間係数の選択にも文脈を考慮するとより尤度の高い補間係数を選ぶことができます。  
> <div style="text-align: center;">
P(w<sub>i</sub> | w<sub>i-1</sub>) = λ<sub>w<sub>i-1</sub></sub>P<sub>ML</sub>(w<sub>i</sub> | w<sub>i-1</sub>) + (1-λ<sub>w<sub>i-1</sub></sub>)P(w<sub>i</sub>)
</div>

このλ<sub>w<sub>i-1</sub></sub>を選ぶ方法の1つに*Witten-Bell Smoothing*があります。Witten-Bell Smootingでは単語列に続く単語の異なり数を考えます。これが多いほど補間係数を大きくなるようにするのです。

><div style="text-align: center;">[tex:{ \displaystyle
\lambda\_{w\_{i-1}} = 1 - \frac{u(w\_{i-1})}{u(w\_{i-1})+c(w\_{i-1})}
}]</div>

このu(w<sub>i-1</sub>)がw<sub>i-1</sub>の後に続く単語の異なり数。
例えば、

<table align="center">
	<tr>
		<td>
c(Tottori is) = 2<br>
c(Tottori city) = 1<br>
c(Tottori) = 3<br>
u(Tottori) = 2
		</td>
		<td>
<div style="text-align: center;">[tex:{ \displaystyle
\lambda\_{Tottori} = 1 - \frac{2}{2+3} = 0.6
}]</div>
		</td>
	</tr>
	<tr>
		<td>
c(Tokyo city) = 40<br>
c(Tokyo is) = 35<br>
<div style="text-align: center;">
・<br>・<br>
</div>
c(Tokyo) = 270<br>
u(Tokyo) = 30<br>
		</td>
		<td>
<div style="text-align: center;">[tex:{ \displaystyle
\lambda\_{Tokyo} = 1 - \frac{30}{30+270} = 0.9
}]</div>
		</td>
	</tr>
</table>

###実装
長ったらしいコードをわざわざ埋め込むこともないなと思い、今回からはGitHubへのリンクだけ張ることにしました。  


なかなか忙しくて(&だらけてて)1記事/weekのペースを実現できていませんがマイペースにやっていきます。  


[Capter02]: http://www.phontron.com/slides/nlp-programming-ja-02-bigramlm.pdf
[小町研究室]: http://cl.sd.tmu.ac.jp
[@Ace12358]: https://twitter.com/Ace12358
[@zawa9510]: https://twitter.com/zawa9510
[自然言語処理のウェザーリポート]: http://ace12358.tumblr.com
[自然言語処理の三ツ星カルテット]: http://yu-zawa.hatenablog.com
